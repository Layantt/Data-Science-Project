{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOFHYw7FqM58AtRsvfSHgG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Layantt/Data-Science-Project/blob/main/DataScience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OFjC5tWif0aL",
        "outputId": "32eeee4b-d4a3-462e-ace3-03768b50fb17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting enhanced data collection with article status detection...\n",
            "‚ùå Error in data collection: HTTPSConnectionPool(host='laws.boe.gov.sa', port=443): Max retries exceeded with url: /boelaws/laws/lawdetails/b7cfae89-828e-4994-b167-adaa00e37188/1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)')))\n",
            "\n",
            "üìÅ Files saved in directory: raw_data\n",
            "‚úÖ Enhanced data collection completed successfully!\n",
            "üì• Files ready for download in the output directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 464, in _make_request\n",
            "    self._validate_conn(conn)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 1093, in _validate_conn\n",
            "    conn.connect()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 790, in connect\n",
            "    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 969, in _ssl_wrap_socket_and_match_hostname\n",
            "    ssl_sock = ssl_wrap_socket(\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/ssl_.py\", line 480, in ssl_wrap_socket\n",
            "    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/ssl_.py\", line 524, in _ssl_wrap_socket_impl\n",
            "    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 455, in wrap_socket\n",
            "    return self.sslsocket_class._create(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1041, in _create\n",
            "    self.do_handshake()\n",
            "  File \"/usr/lib/python3.12/ssl.py\", line 1319, in do_handshake\n",
            "    self._sslobj.do_handshake()\n",
            "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 488, in _make_request\n",
            "    raise new_e\n",
            "urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='laws.boe.gov.sa', port=443): Max retries exceeded with url: /boelaws/laws/lawdetails/b7cfae89-828e-4994-b167-adaa00e37188/1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)')))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2837688279.py\", line 36, in <cell line: 0>\n",
            "    response = requests.get(url, headers=headers, timeout=30)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/api.py\", line 73, in get\n",
            "    return request(\"get\", url, params=params, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/api.py\", line 59, in request\n",
            "    return session.request(method=method, url=url, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/requests/adapters.py\", line 698, in send\n",
            "    raise SSLError(e, request=request)\n",
            "requests.exceptions.SSLError: HTTPSConnectionPool(host='laws.boe.gov.sa', port=443): Max retries exceeded with url: /boelaws/laws/lawdetails/b7cfae89-828e-4994-b167-adaa00e37188/1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)')))\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries Installation\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    packages = [\"requests\", \"beautifulsoup4\", \"pandas\", \"lxml\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package.replace(\"beautifulsoup4\", \"bs4\"))\n",
        "        except ImportError:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], check=True)\n",
        "\n",
        "install_packages()\n",
        "\n",
        "# Import Libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# Create Output Directory\n",
        "output_dir = \"raw_data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Saudi Personal Data Protection Law URL\n",
        "url = \"https://laws.boe.gov.sa/boelaws/laws/lawdetails/b7cfae89-828e-4994-b167-adaa00e37188/1\"\n",
        "\n",
        "print(\"Starting enhanced data collection with article status detection...\")\n",
        "\n",
        "try:\n",
        "    # Fetch Web Page\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "    response = requests.get(url, headers=headers, timeout=30)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Save Raw HTML (Unstructured Data)\n",
        "    html_file = os.path.join(output_dir, \"pdpl_raw_html.html\")\n",
        "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"Raw HTML saved: {html_file}\")\n",
        "\n",
        "    # Parse HTML with BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, \"lxml\")\n",
        "\n",
        "    # NEW: Extract articles using HTML structure first (if available)\n",
        "    articles_from_html = []\n",
        "    article_divs = soup.find_all(\"div\", class_=\"article_item\")\n",
        "\n",
        "    if article_divs:\n",
        "        print(f\"Found {len(article_divs)} article divs in HTML structure\")\n",
        "\n",
        "        for div in article_divs:\n",
        "            # Determine article status based on CSS classes\n",
        "            css_classes = div.get(\"class\", [])\n",
        "\n",
        "            # Check if article is modified\n",
        "            if \"changed-article\" in css_classes:\n",
        "                article_status = \"ŸÖÿπÿØŸÑÿ©\"\n",
        "            elif \"no_alternate\" in css_classes:\n",
        "                article_status = \"ÿ£ÿµŸÑŸäÿ©\"\n",
        "            else:\n",
        "                article_status = \"ÿ∫Ÿäÿ± ŸÖÿ≠ÿØÿØ\"\n",
        "\n",
        "            # Extract article title\n",
        "            title_element = div.find(\"h3\")\n",
        "            article_title = title_element.get_text(strip=True) if title_element else \"ÿπŸÜŸàÿßŸÜ ÿ∫Ÿäÿ± ŸÖÿ≠ÿØÿØ\"\n",
        "\n",
        "            # Extract article content\n",
        "            content_div = div.find(\"div\", class_=\"HTMLContainer\")\n",
        "            if content_div:\n",
        "                article_text = content_div.get_text(separator=\" \", strip=True)\n",
        "\n",
        "                # NEW: Extract modification text from popup if exists\n",
        "                modification_text = \"ŸÑÿß ŸäŸàÿ¨ÿØ ŸÜÿµ ŸÖÿπÿØŸÑ\"  # Default for original articles\n",
        "\n",
        "                if article_status == \"ŸÖÿπÿØŸÑÿ©\":\n",
        "                    # Look for article_item_popup within this article div\n",
        "                    popup_div = div.find(\"div\", class_=\"article_item_popup\")\n",
        "                    if popup_div:\n",
        "                        popup_content = popup_div.find(\"div\", class_=\"HTMLContainer\")\n",
        "                        if popup_content:\n",
        "                            popup_text = popup_content.get_text(separator=\" \", strip=True)\n",
        "                            if popup_text and len(popup_text) > 10:\n",
        "                                modification_text = popup_text\n",
        "                        else:\n",
        "                            # If no HTMLContainer in popup, get all text from popup\n",
        "                            popup_text = popup_div.get_text(separator=\" \", strip=True)\n",
        "                            if popup_text and len(popup_text) > 10:\n",
        "                                modification_text = popup_text\n",
        "\n",
        "                # Clean content from administrative text\n",
        "                if len(article_text) > 10:  # Ensure meaningful content\n",
        "                    articles_from_html.append({\n",
        "                        \"article_title\": article_title,\n",
        "                        \"article_text\": article_text,\n",
        "                        \"article_status\": article_status,\n",
        "                        \"modification_text\": modification_text\n",
        "                    })\n",
        "\n",
        "    # Fallback: Extract from text if no HTML structure found\n",
        "    articles_from_text = []\n",
        "    if not articles_from_html:\n",
        "        print(\"No structured HTML found, falling back to text extraction...\")\n",
        "\n",
        "        text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "        lines = text.split(\"\\n\")\n",
        "        current_article = None\n",
        "        current_content = []\n",
        "        article_found = False\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Check for New Article Header\n",
        "            if re.match(r\"^ÿßŸÑŸÖÿßÿØÿ©\\s+\", line):\n",
        "                # Save Previous Article if exists\n",
        "                if current_article and current_content and article_found:\n",
        "                    article_text = \" \".join(current_content).strip()\n",
        "                    if len(article_text) > 10:\n",
        "\n",
        "                        # Determine status from text content\n",
        "                        article_status = \"ÿ£ÿµŸÑŸäÿ©\"  # Default\n",
        "                        modification_text = \"ŸÑÿß ŸäŸàÿ¨ÿØ ŸÜÿµ ŸÖÿπÿØŸÑ\"  # Default for original articles\n",
        "\n",
        "                        if any(indicator in article_text for indicator in [\n",
        "                            \"ÿπŸèÿØŸÑÿ™ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿßÿØÿ©\", \"ÿßŸÑŸÖÿ±ÿ≥ŸàŸÖ ÿßŸÑŸÖŸÑŸÉŸä\", \"ÿ®ŸÖŸàÿ¨ÿ®\", \"ŸÖÿπÿØŸÑÿ©\"\n",
        "                        ]):\n",
        "                            article_status = \"ŸÖÿπÿØŸÑÿ©\"\n",
        "                            # Try to extract modification text from content\n",
        "                            modification_patterns = [\n",
        "                                r'ÿπŸèÿØŸÑÿ™ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿßÿØÿ©.*?\"(.*?)\"',\n",
        "                                r'ÿ®ŸÖŸàÿ¨ÿ®.*?ÿßŸÑŸÖÿ±ÿ≥ŸàŸÖ ÿßŸÑŸÖŸÑŸÉŸä.*?(.*?)(?:\\.|$)',\n",
        "                                r'ŸÑÿ™ŸÉŸàŸÜ ÿ®ÿßŸÑŸÜÿµ ÿßŸÑÿ¢ÿ™Ÿä.*?\"(.*?)\"'\n",
        "                            ]\n",
        "\n",
        "                            for pattern in modification_patterns:\n",
        "                                match = re.search(pattern, article_text, re.DOTALL)\n",
        "                                if match:\n",
        "                                    modification_text = match.group(1).strip()\n",
        "                                    break\n",
        "\n",
        "                            # If no specific pattern found, use general modification text\n",
        "                            if modification_text == \"ŸÑÿß ŸäŸàÿ¨ÿØ ŸÜÿµ ŸÖÿπÿØŸÑ\":\n",
        "                                mod_start = article_text.find(\"ÿπŸèÿØŸÑÿ™ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿßÿØÿ©\")\n",
        "                                if mod_start != -1:\n",
        "                                    modification_text = article_text[mod_start:mod_start+200] + \"...\"\n",
        "\n",
        "                        articles_from_text.append({\n",
        "                            \"article_title\": current_article,\n",
        "                            \"article_text\": article_text,\n",
        "                            \"article_status\": article_status,\n",
        "                            \"modification_text\": modification_text\n",
        "                        })\n",
        "\n",
        "                # Start New Article\n",
        "                current_article = line\n",
        "                current_content = []\n",
        "                article_found = True\n",
        "\n",
        "            else:\n",
        "                # Add Content to Current Article\n",
        "                if current_article and article_found:\n",
        "                    # Smart Filtering\n",
        "                    line_lower = line.lower().strip()\n",
        "\n",
        "                    # Skip Very Short Lines\n",
        "                    if len(line) <= 3:\n",
        "                        continue\n",
        "\n",
        "                    # Skip Administrative Lines\n",
        "                    skip_line = False\n",
        "                    if (line.startswith(\"ÿ™ÿßÿ±ŸäÿÆ\") or\n",
        "                        line.startswith(\"ÿ±ŸÇŸÖ\") or\n",
        "                        line.startswith(\"ÿßŸÑÿ¨ÿ±ŸäÿØÿ© ÿßŸÑÿ±ÿ≥ŸÖŸäÿ©\") or\n",
        "                        line.startswith(\"*\") or\n",
        "                        line.startswith(\"-\")):\n",
        "                        skip_line = True\n",
        "\n",
        "                    # Administrative patterns\n",
        "                    admin_only_patterns = [\n",
        "                        \"ÿ™ÿπÿØŸäŸÑÿßÿ™ ÿßŸÑŸÖÿßÿØÿ©\",\n",
        "                        \"ŸÖÿßÿØÿ© ŸÖÿπÿØŸÑÿ©\",\n",
        "                        \"ŸÖÿßÿØÿ© ŸÖŸÑÿ∫Ÿäÿ©\"\n",
        "                    ]\n",
        "\n",
        "                    for pattern in admin_only_patterns:\n",
        "                        if (pattern in line_lower and\n",
        "                            len(line.strip()) < 150 and\n",
        "                            line.count('.') <= 1):\n",
        "                            skip_line = True\n",
        "                            break\n",
        "\n",
        "                    if not skip_line:\n",
        "                        current_content.append(line)\n",
        "\n",
        "        # Save Last Article\n",
        "        if current_article and current_content and article_found:\n",
        "            article_text = \" \".join(current_content).strip()\n",
        "            if len(article_text) > 10:\n",
        "                # Determine status from text content\n",
        "                article_status = \"ÿ£ÿµŸÑŸäÿ©\"  # Default\n",
        "                modification_text = \"ŸÑÿß ŸäŸàÿ¨ÿØ ŸÜÿµ ŸÖÿπÿØŸÑ\"  # Default for original articles\n",
        "\n",
        "                if any(indicator in article_text for indicator in [\n",
        "                    \"ÿπŸèÿØŸÑÿ™ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿßÿØÿ©\", \"ÿßŸÑŸÖÿ±ÿ≥ŸàŸÖ ÿßŸÑŸÖŸÑŸÉŸä\", \"ÿ®ŸÖŸàÿ¨ÿ®\", \"ŸÖÿπÿØŸÑÿ©\"\n",
        "                ]):\n",
        "                    article_status = \"ŸÖÿπÿØŸÑÿ©\"\n",
        "                    # Try to extract modification text from content\n",
        "                    modification_patterns = [\n",
        "                        r'ÿπŸèÿØŸÑÿ™ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿßÿØÿ©.*?\"(.*?)\"',\n",
        "                        r'ÿ®ŸÖŸàÿ¨ÿ®.*?ÿßŸÑŸÖÿ±ÿ≥ŸàŸÖ ÿßŸÑŸÖŸÑŸÉŸä.*?(.*?)(?:\\.|$)',\n",
        "                        r'ŸÑÿ™ŸÉŸàŸÜ ÿ®ÿßŸÑŸÜÿµ ÿßŸÑÿ¢ÿ™Ÿä.*?\"(.*?)\"'\n",
        "                    ]\n",
        "\n",
        "                    for pattern in modification_patterns:\n",
        "                        match = re.search(pattern, article_text, re.DOTALL)\n",
        "                        if match:\n",
        "                            modification_text = match.group(1).strip()\n",
        "                            break\n",
        "\n",
        "                    # If no specific pattern found, use general modification text\n",
        "                    if modification_text == \"ŸÑÿß ŸäŸàÿ¨ÿØ ŸÜÿµ ŸÖÿπÿØŸÑ\":\n",
        "                        mod_start = article_text.find(\"ÿπŸèÿØŸÑÿ™ Ÿáÿ∞Ÿá ÿßŸÑŸÖÿßÿØÿ©\")\n",
        "                        if mod_start != -1:\n",
        "                            modification_text = article_text[mod_start:mod_start+200] + \"...\"\n",
        "\n",
        "                articles_from_text.append({\n",
        "                    \"article_title\": current_article,\n",
        "                    \"article_text\": article_text,\n",
        "                    \"article_status\": article_status,\n",
        "                    \"modification_text\": modification_text\n",
        "                })\n",
        "\n",
        "    # Use HTML extraction if available, otherwise use text extraction\n",
        "    articles = articles_from_html if articles_from_html else articles_from_text\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(articles)\n",
        "\n",
        "    # Data Cleaning\n",
        "    if not df.empty:\n",
        "        df = df[df[\"article_text\"].str.len() > 10]  # Meaningful content\n",
        "        df = df.drop_duplicates(subset=[\"article_title\"])  # Remove duplicates\n",
        "\n",
        "        # Special handling for first article duplicates\n",
        "        first_articles = df[df[\"article_title\"].str.contains(\"ÿßŸÑŸÖÿßÿØÿ© ÿßŸÑÿ£ŸàŸÑŸâ|ÿßŸÑŸÖÿßÿØÿ© \\\\(1\\\\)|ÿßŸÑŸÖÿßÿØÿ© Ÿ°\", case=False, na=False, regex=True)]\n",
        "        if len(first_articles) > 1:\n",
        "            print(\"Detected duplicate first article - fixing...\")\n",
        "            best_first = first_articles.loc[first_articles[\"article_text\"].str.len().idxmax()]\n",
        "            df = df[~df[\"article_title\"].str.contains(\"ÿßŸÑŸÖÿßÿØÿ© ÿßŸÑÿ£ŸàŸÑŸâ|ÿßŸÑŸÖÿßÿØÿ© \\\\(1\\\\)|ÿßŸÑŸÖÿßÿØÿ© Ÿ°\", case=False, na=False, regex=True)]\n",
        "            df = pd.concat([pd.DataFrame([best_first]), df]).reset_index(drop=True)\n",
        "\n",
        "    # Save Enhanced Structured Data\n",
        "    csv_file = os.path.join(output_dir, \"pdpl_articles_enhanced.csv\")\n",
        "    df.to_csv(csv_file, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"Successfully extracted {len(df)} articles with status information!\")\n",
        "    print(f\"Enhanced data saved to: {csv_file}\")\n",
        "\n",
        "    # Display Statistics\n",
        "    if not df.empty:\n",
        "        print(\"\\nüìä Article Status Summary:\")\n",
        "        print(\"-\" * 40)\n",
        "        status_counts = df['article_status'].value_counts()\n",
        "        for status, count in status_counts.items():\n",
        "            print(f\"‚Ä¢ {status}: {count} ŸÖÿßÿØÿ©\")\n",
        "\n",
        "        print(f\"\\nüìù Total Articles: {len(df)}\")\n",
        "\n",
        "        # Display Sample Data\n",
        "        print(\"\\nüîç Sample of collected data:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, row in df.head(3).iterrows():\n",
        "            status_emoji = \"‚úèÔ∏è\" if row['article_status'] == \"ŸÖÿπÿØŸÑÿ©\" else \"üìã\"\n",
        "            print(f\"{status_emoji} {row['article_title']} ({row['article_status']})\")\n",
        "            print(f\"   Content: {row['article_text'][:100]}...\")\n",
        "            print(f\"   Modification: {row['modification_text'][:100]}{'...' if len(row['modification_text']) > 100 else ''}\")\n",
        "            print()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in data collection: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(f\"\\nüìÅ Files saved in directory: {output_dir}\")\n",
        "print(\"‚úÖ Enhanced data collection completed successfully!\")\n",
        "\n",
        "# Download files (for Google Colab)\n",
        "try:\n",
        "    files.download(f'{output_dir}/pdpl_raw_html.html')\n",
        "    files.download(f'{output_dir}/pdpl_articles_enhanced.csv')\n",
        "except:\n",
        "    print(\"üì• Files ready for download in the output directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Processing and Cleaning"
      ],
      "metadata": {
        "id": "nFpcK20v2nqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the structured dataset extracted from the scraping code\n",
        "df = pd.read_csv(\"raw_data/pdpl_articles_enhanced.csv\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"Initial shape:\", df.shape)\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "0VsXB-U53IJ3",
        "outputId": "d063261f-5b7c-4269-f3cf-272060d9d814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'raw_data/pdpl_articles_enhanced.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1915056521.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the structured dataset extracted from the scraping code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw_data/pdpl_articles_enhanced.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Display basic info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'raw_data/pdpl_articles_enhanced.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = df.duplicated(subset=[\"article_title\"]).sum()\n",
        "print(f\"\\nDuplicate articles found: {duplicates}\")\n",
        "\n",
        "# Display sample\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "MrHnk_JU3mBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove extra spaces and newlines\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove non-Arabic or special symbols if any\n",
        "    text = re.sub(r'[^\\u0600-\\u06FF\\s.,ÿõ:ÿå]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply cleaning\n",
        "df[\"article_text\"] = df[\"article_text\"].apply(clean_text)\n",
        "df[\"modification_text\"] = df[\"modification_text\"].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "E8T0xNJP3rNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize status column to ensure consistency\n",
        "df[\"article_status\"] = df[\"article_status\"].replace({\n",
        "    \"ŸÖÿπÿØŸÑÿ©\": \"Modified\",\n",
        "    \"ÿ£ÿµŸÑŸäÿ©\": \"Original\",\n",
        "    \"ÿ∫Ÿäÿ± ŸÖÿ≠ÿØÿØ\": \"Unknown\"\n",
        "})\n"
      ],
      "metadata": {
        "id": "FxGzWVHt3xeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned and processed dataset\n",
        "cleaned_file = \"raw_data/pdpl_cleaned.csv\"\n",
        "df.to_csv(cleaned_file, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(f\"‚úÖ Cleaned dataset saved to: {cleaned_file}\")\n"
      ],
      "metadata": {
        "id": "GG8cWbk53yXM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}