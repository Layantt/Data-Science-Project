{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPuCZti2Mx4vi1ZCzucs7rM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Layantt/Data-Science-Project/blob/main/DataScience.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFjC5tWif0aL"
      },
      "outputs": [],
      "source": [
        "# Required Libraries Installation\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    packages = [\"requests\", \"beautifulsoup4\", \"pandas\", \"lxml\"]\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package.replace(\"beautifulsoup4\", \"bs4\"))\n",
        "        except ImportError:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], check=True)\n",
        "\n",
        "install_packages()\n",
        "\n",
        "# Import Libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "# Create Output Directory\n",
        "output_dir = \"raw_data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Saudi Personal Data Protection Law URL\n",
        "url = \"https://laws.boe.gov.sa/boelaws/laws/lawdetails/b7cfae89-828e-4994-b167-adaa00e37188/1\"\n",
        "\n",
        "print(\"Starting data collection from official source...\")\n",
        "\n",
        "try:\n",
        "    # Fetch Web Page\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "    response = requests.get(url, headers=headers, timeout=30)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Save Raw HTML (Unstructured Data)\n",
        "    html_file = os.path.join(output_dir, \"pdpl_raw_html.html\")\n",
        "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "    print(f\"Raw HTML saved: {html_file}\")\n",
        "\n",
        "    # Parse HTML and Extract Articles\n",
        "    soup = BeautifulSoup(response.text, \"lxml\")\n",
        "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "    # Article Extraction using Regex\n",
        "    articles = []\n",
        "\n",
        "    # Process Text Line by Line\n",
        "    lines = text.split(\"\\n\")\n",
        "    current_article = None\n",
        "    current_content = []\n",
        "    article_found = False  # Track if we started processing articles\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Check for New Article Header\n",
        "        if re.match(r\"^المادة\\s+\", line):\n",
        "            # Save Previous Article if exists\n",
        "            if current_article and current_content and article_found:\n",
        "                article_text = \" \".join(current_content).strip()\n",
        "                if len(article_text) > 10:  # Ensure meaningful content\n",
        "                    articles.append({\n",
        "                        \"article_title\": current_article,\n",
        "                        \"article_text\": article_text,\n",
        "                        \"source_url\": url,\n",
        "                        \"collection_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                    })\n",
        "\n",
        "            # Start New Article\n",
        "            current_article = line\n",
        "            current_content = []\n",
        "            article_found = True\n",
        "\n",
        "        else:\n",
        "            # Add Content to Current Article\n",
        "            if current_article and article_found:\n",
        "                # Smart Filtering - Remove Administrative Content Only\n",
        "                line_lower = line.lower().strip()\n",
        "\n",
        "                # Skip Very Short Lines\n",
        "                if len(line) <= 3:\n",
        "                    continue\n",
        "\n",
        "                # Check if line should be skipped\n",
        "                skip_line = False\n",
        "\n",
        "                # Clear Administrative Lines\n",
        "                if (line.startswith(\"تاريخ\") or\n",
        "                    line.startswith(\"رقم\") or\n",
        "                    line.startswith(\"الجريدة الرسمية\") or\n",
        "                    line.startswith(\"*\") or\n",
        "                    line.startswith(\"-\")):\n",
        "                    skip_line = True\n",
        "\n",
        "                # Amendment Boxes (Pure Administrative Information)\n",
        "                admin_only_patterns = [\n",
        "                    \"تعديلات المادة\",\n",
        "                    \"مادة معدلة\",\n",
        "                    \"مادة ملغية\",\n",
        "                    \"معدلة بموجب المرسوم الملكي\",\n",
        "                    \"ملغاة بموجب المرسوم الملكي\",\n",
        "                    \"وتاريخ\"\n",
        "                ]\n",
        "\n",
        "                # Check for Pure Administrative Content\n",
        "                for pattern in admin_only_patterns:\n",
        "                    if (pattern in line_lower and\n",
        "                        len(line.strip()) < 150 and  # Short line\n",
        "                        line.count('.') <= 1):       # Not complex legal text\n",
        "                        skip_line = True\n",
        "                        break\n",
        "\n",
        "                # Add Line if Not Pure Administrative\n",
        "                if not skip_line:\n",
        "                    current_content.append(line)\n",
        "\n",
        "    # Save Last Article\n",
        "    if current_article and current_content and article_found:\n",
        "        article_text = \" \".join(current_content).strip()\n",
        "        if len(article_text) > 10:\n",
        "            articles.append({\n",
        "                \"article_title\": current_article,\n",
        "                \"article_text\": article_text,\n",
        "                \"source_url\": url,\n",
        "                \"collection_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(articles)\n",
        "\n",
        "    # Data Cleaning - Remove Empty or Duplicate Articles\n",
        "    df = df[df[\"article_text\"].str.len() > 10]  # Articles with meaningful content\n",
        "    df = df.drop_duplicates(subset=[\"article_title\"])  # Remove duplicates\n",
        "\n",
        "    # Special Check for First Article - Remove Duplicates\n",
        "    first_articles = df[df[\"article_title\"].str.contains(\"المادة الأولى|المادة \\\\(1\\\\)|المادة ١\", case=False, na=False, regex=True)]\n",
        "    if len(first_articles) > 1:\n",
        "        print(\"Detected duplicate first article - fixing...\")\n",
        "        # Keep the article with longest text (most complete)\n",
        "        best_first = first_articles.loc[first_articles[\"article_text\"].str.len().idxmax()]\n",
        "        # Remove all first articles and replace with best one\n",
        "        df = df[~df[\"article_title\"].str.contains(\"المادة الأولى|المادة \\\\(1\\\\)|المادة ١\", case=False, na=False, regex=True)]\n",
        "        df = pd.concat([pd.DataFrame([best_first]), df]).reset_index(drop=True)\n",
        "\n",
        "    # Save Structured Data\n",
        "    csv_file = os.path.join(output_dir, \"pdpl_articles.csv\")\n",
        "    df.to_csv(csv_file, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"Successfully extracted {len(df)} articles and saved to: {csv_file}\")\n",
        "\n",
        "    # Display Data Sample\n",
        "    print(\"\\nSample of collected data:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, row in df.head(3).iterrows():\n",
        "        print(f\"• {row['article_title']}\")\n",
        "        print(f\"   Content: {row['article_text'][:100]}...\")\n",
        "        print()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in data collection: {str(e)}\")\n",
        "\n",
        "print(f\"\\nFiles saved in directory: {output_dir}\")\n",
        "print(\"Data collection completed successfully!\")\n",
        "files.download('/content/raw_data/pdpl_raw_html.html')\n",
        "files.download('/content/raw_data/pdpl_articles.csv')"
      ]
    }
  ]
}